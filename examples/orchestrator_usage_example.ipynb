{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d499a05",
   "metadata": {},
   "source": [
    "# Financial News RAG Orchestrator Usage Example\n",
    "\n",
    "This notebook demonstrates how to use the `FinancialNewsRAG` orchestrator class, which provides a high-level interface to the financial-news-rag system. The orchestrator integrates all the low-level components:\n",
    "\n",
    "- EODHD API client for fetching financial news articles\n",
    "- Article Manager for storing and retrieving articles from SQLite\n",
    "- Text Processor for cleaning and chunking article content\n",
    "- Embeddings Generator for creating embeddings from text chunks\n",
    "- ChromaDB Manager for storing and querying vector embeddings\n",
    "- ReRanker for improving search results using Gemini LLM\n",
    "\n",
    "This example will walk through the complete pipeline from fetching articles to searching and retrieving relevant content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde193ec",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, we need to import the necessary modules and initialize the orchestrator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41158fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "\n",
    "# Add the project root to the path if needed\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "# Import the orchestrator\n",
    "from financial_news_rag.orchestrator import FinancialNewsRAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016e8bc1",
   "metadata": {},
   "source": [
    "### Load Environment Variables\n",
    "\n",
    "The orchestrator requires API keys for the EODHD API and Gemini API. These should be set in a `.env` file in the project root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8aadfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Check for required API keys\n",
    "eodhd_api_key = os.getenv(\"EODHD_API_KEY\")\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "if not eodhd_api_key:\n",
    "    print(\"⚠️ EODHD_API_KEY not found in environment variables.\")\n",
    "    print(\"Please create a .env file with your EODHD_API_KEY.\")\n",
    "else:\n",
    "    print(\"✅ EODHD_API_KEY found.\")\n",
    "    \n",
    "if not gemini_api_key:\n",
    "    print(\"⚠️ GEMINI_API_KEY not found in environment variables.\")\n",
    "    print(\"Please create a .env file with your GEMINI_API_KEY.\")\n",
    "else:\n",
    "    print(\"✅ GEMINI_API_KEY found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654ae2ad",
   "metadata": {},
   "source": [
    "### Initialize the Orchestrator\n",
    "\n",
    "Now we can create an instance of the `FinancialNewsRAG` orchestrator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294f2f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the orchestrator with default settings\n",
    "# This will use the API keys from environment variables\n",
    "# and default paths for the databases\n",
    "try:\n",
    "    rag = FinancialNewsRAG(\n",
    "        # Optional: override API keys if needed\n",
    "        # eodhd_api_key=eodhd_api_key,\n",
    "        # gemini_api_key=gemini_api_key,\n",
    "        \n",
    "        # Optional: customize database paths\n",
    "        # db_path=\"custom_financial_news.db\",  # SQLite database path\n",
    "        # chroma_persist_dir=\"custom_chroma_db\",  # ChromaDB persistence directory\n",
    "        \n",
    "        # Optional: other settings\n",
    "        # chroma_collection_name=\"custom_collection\",  # Name for the ChromaDB collection\n",
    "        max_tokens_per_chunk=2048,  # Maximum tokens per text chunk\n",
    "    )\n",
    "    print(\"✅ FinancialNewsRAG orchestrator initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error initializing orchestrator: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff3435c",
   "metadata": {},
   "source": [
    "## 1. Fetching and Storing Articles\n",
    "\n",
    "The first step in the RAG pipeline is to fetch articles from the EODHD API and store them in the SQLite database. We can fetch articles by tag or symbol."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f3ad71",
   "metadata": {},
   "source": [
    "### Fetch Articles by Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ae537c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define date range for the fetch operation\n",
    "today = datetime.now()\n",
    "one_week_ago = today - timedelta(days=7)\n",
    "\n",
    "# Format dates as YYYY-MM-DD\n",
    "from_date = one_week_ago.strftime(\"%Y-%m-%d\")\n",
    "to_date = today.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a1b2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Fetching technology news articles from {from_date} to {to_date}...\")\n",
    "\n",
    "# Fetch articles with the TECHNOLOGY tag\n",
    "result = rag.fetch_and_store_articles(\n",
    "    tag=\"TECHNOLOGY\",  # News category tag\n",
    "    from_date=from_date,\n",
    "    to_date=to_date,\n",
    "    limit=10  # Maximum number of articles to fetch\n",
    ")\n",
    "\n",
    "print(f\"\\nFetch operation completed with status: {result['status']}\")\n",
    "print(f\"Articles fetched: {result['articles_fetched']}\")\n",
    "print(f\"Articles stored: {result['articles_stored']}\")\n",
    "\n",
    "if result['errors']:\n",
    "    print(\"\\nErrors encountered:\")\n",
    "    for error in result['errors']:\n",
    "        print(f\"- {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1445b9f1",
   "metadata": {},
   "source": [
    "### Fetch Articles by Symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d3d933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch articles for multiple stock symbols\n",
    "print(f\"Fetching news for tech stocks from {from_date} to {to_date}...\")\n",
    "\n",
    "result = rag.fetch_and_store_articles(\n",
    "    symbol=\"AAPL.US\",  # Comma-separated list of symbols\n",
    "    from_date=from_date,\n",
    "    to_date=to_date,\n",
    "    limit=20  # Maximum number of articles to fetch\n",
    ")\n",
    "\n",
    "print(f\"\\nFetch operation completed with status: {result['status']}\")\n",
    "print(f\"Articles fetched: {result['articles_fetched']}\")\n",
    "print(f\"Articles stored: {result['articles_stored']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a186cf8",
   "metadata": {},
   "source": [
    "## 2. Processing Article Text\n",
    "\n",
    "After fetching and storing the articles, we need to process the raw content to clean and normalize the text for embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b7ee03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process pending articles (those with status_text_processing = 'PENDING')\n",
    "print(\"Processing pending articles...\")\n",
    "\n",
    "result = rag.process_pending_articles(limit=5)  # Number of pending articles to process\n",
    "\n",
    "print(f\"\\nProcessing completed with status: {result['status']}\")\n",
    "print(f\"Articles successfully processed: {result['articles_processed']}\")\n",
    "print(f\"Articles that failed processing: {result['articles_failed']}\")\n",
    "\n",
    "if result['errors']:\n",
    "    print(\"\\nErrors encountered during processing:\")\n",
    "    for error in result['errors']:\n",
    "        print(f\"- {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c8384e",
   "metadata": {},
   "source": [
    "### Reprocess Articles with Failed Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6d1b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get any articles that failed text processing\n",
    "failed_articles = rag.get_failed_text_processing_articles()\n",
    "print(f\"Found {len(failed_articles)} articles with failed text processing.\")\n",
    "\n",
    "if failed_articles:\n",
    "    print(\"\\nAttempting to reprocess failed articles...\")\n",
    "    result = rag.reprocess_failed_articles()\n",
    "    \n",
    "    print(f\"\\nReprocessing completed with status: {result['status']}\")\n",
    "    print(f\"Articles successfully reprocessed: {result['articles_reprocessed']}\")\n",
    "    print(f\"Articles that failed reprocessing: {result['articles_failed']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22bde28",
   "metadata": {},
   "source": [
    "## 3. Generating and Storing Embeddings\n",
    "\n",
    "Now that we have processed articles, we can generate embeddings for them and store them in ChromaDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f173e7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for processed articles\n",
    "print(\"Generating embeddings for processed articles...\")\n",
    "\n",
    "result = rag.embed_processed_articles(limit=10)  # Number of processed articles to embed\n",
    "\n",
    "print(f\"\\nEmbedding generation completed with status: {result['status']}\")\n",
    "print(f\"Articles successfully embedded: {result['articles_embedded']}\")\n",
    "print(f\"Articles that failed embedding: {result['articles_failed']}\")\n",
    "\n",
    "if result['errors']:\n",
    "    print(\"\\nErrors encountered during embedding:\")\n",
    "    for error in result['errors']:\n",
    "        print(f\"- {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eff8fc2",
   "metadata": {},
   "source": [
    "### Re-embed Articles with Failed Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac1b11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get any articles that failed embedding\n",
    "failed_articles = rag.get_failed_embedding_articles()\n",
    "print(f\"Found {len(failed_articles)} articles with failed embedding.\")\n",
    "\n",
    "if failed_articles:\n",
    "    print(\"\\nAttempting to re-embed failed articles...\")\n",
    "    result = rag.re_embed_failed_articles()\n",
    "    \n",
    "    print(f\"\\nRe-embedding completed with status: {result['status']}\")\n",
    "    print(f\"Articles successfully re-embedded: {result['articles_reembedded']}\")\n",
    "    print(f\"Articles that failed re-embedding: {result['articles_failed']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e603b1e",
   "metadata": {},
   "source": [
    "## 4. Checking Database Status\n",
    "\n",
    "We can check the status of both the article database (SQLite) and the vector database (ChromaDB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cdffb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get article database status\n",
    "print(\"Checking article database status...\")\n",
    "article_db_status = rag.get_article_database_status()\n",
    "\n",
    "print(f\"\\nArticle Database Status:\")\n",
    "print(f\"Total articles: {article_db_status['total_articles']}\")\n",
    "\n",
    "print(\"\\nText processing status:\")\n",
    "for status, count in article_db_status['text_processing_status'].items():\n",
    "    print(f\"  {status}: {count}\")\n",
    "\n",
    "print(\"\\nEmbedding status:\")\n",
    "for status, count in article_db_status['embedding_status'].items():\n",
    "    print(f\"  {status}: {count}\")\n",
    "\n",
    "print(\"\\nArticles by tag:\")\n",
    "for tag, count in article_db_status['articles_by_tag'].items():\n",
    "    print(f\"  {tag}: {count}\")\n",
    "\n",
    "print(\"\\nArticles by symbol:\")\n",
    "for symbol, count in article_db_status['articles_by_symbol'].items():\n",
    "    print(f\"  {symbol}: {count}\")\n",
    "\n",
    "print(f\"\\nDate range: {article_db_status['date_range']['oldest_article']} to {article_db_status['date_range']['newest_article']}\")\n",
    "print(f\"API calls: {article_db_status['api_calls']['total_calls']} (retrieved {article_db_status['api_calls']['total_articles_retrieved']} articles total)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179a4510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vector database status\n",
    "print(\"Checking vector database status...\")\n",
    "vector_db_status = rag.get_vector_database_status()\n",
    "\n",
    "print(f\"\\nVector Database Status:\")\n",
    "print(f\"Collection name: {vector_db_status['collection_name']}\")\n",
    "print(f\"Total chunks: {vector_db_status['total_chunks']}\")\n",
    "print(f\"Unique articles: {vector_db_status['unique_articles']}\")\n",
    "print(f\"Persistence location: {vector_db_status['persist_directory']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9170dcfd",
   "metadata": {},
   "source": [
    "## 5. Searching for Articles\n",
    "\n",
    "Now that we have articles stored and embedded, we can search for relevant content using natural language queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2747b31f",
   "metadata": {},
   "source": [
    "### Basic Search (No Re-ranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7bf501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a basic search using semantic similarity only\n",
    "query = \"Latest artificial intelligence innovations in tech companies\"\n",
    "print(f\"Searching with query: '{query}'\")\n",
    "\n",
    "results = rag.search_articles(\n",
    "    query=query,\n",
    "    n_results=3,  # Return top 3 results\n",
    "    rerank=False  # No re-ranking\n",
    ")\n",
    "\n",
    "print(f\"\\nFound {len(results)} relevant articles:\\n\")\n",
    "\n",
    "for i, article in enumerate(results):\n",
    "    print(f\"Result {i+1}: {article.get('title', 'Untitled')}\")\n",
    "    print(f\"  URL: {article.get('url', 'No URL')}\")\n",
    "    print(f\"  Published: {article.get('published_at', 'Unknown date')}\")\n",
    "    print(f\"  Similarity score: {article.get('similarity_score', 0):.4f}\")\n",
    "    \n",
    "    # Print a short preview of the content\n",
    "    content = article.get('processed_content', 'No content')\n",
    "    preview = content[:300] + '...' if len(content) > 300 else content\n",
    "    print(f\"  Preview: {preview}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bab9d4",
   "metadata": {},
   "source": [
    "### Search with Re-ranking\n",
    "\n",
    "Re-ranking uses the Gemini LLM to improve search results by considering semantic meaning beyond vector similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaab808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search with re-ranking using Gemini LLM\n",
    "query = \"Impact of generative AI on financial markets\"\n",
    "print(f\"Searching with query: '{query}' (with re-ranking)\")\n",
    "preview_length = 500  # Length of the preview to display\n",
    "\n",
    "results = rag.search_articles(\n",
    "    query=query,\n",
    "    n_results=3,  # Return top 3 results\n",
    "    rerank=True   # Apply re-ranking\n",
    ")\n",
    "\n",
    "print(f\"\\nFound {len(results)} relevant articles:\\n\")\n",
    "\n",
    "for i, article in enumerate(results):\n",
    "    print(f\"Result {i+1}: {article.get('title', 'Untitled')}\")\n",
    "    print(f\"  URL: {article.get('url', 'No URL')}\")\n",
    "    print(f\"  Published: {article.get('published_at', 'Unknown date')}\")\n",
    "    print(f\"  Similarity score: {article.get('similarity_score', 0):.4f}\")\n",
    "    print(f\"  Re-rank score: {article.get('rerank_score', 0):.4f}\")\n",
    "    \n",
    "    # Print a short preview of the content\n",
    "    content = article.get('processed_content', 'No content')\n",
    "    preview = content[:preview_length] + '...' if len(content) > preview_length else content\n",
    "    print(f\"  Preview: {preview}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb6261b",
   "metadata": {},
   "source": [
    "### Compare Search Results: With and Without Re-ranking\n",
    "\n",
    "Let's see how re-ranking affects the order and quality of search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55298a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare search results with and without re-ranking\n",
    "query = \"AI applications in financial forecasting\"\n",
    "print(f\"Comparing search results for query: '{query}'\")\n",
    "\n",
    "# Search without re-ranking\n",
    "basic_results = rag.search_articles(\n",
    "    query=query,\n",
    "    n_results=5,\n",
    "    rerank=False\n",
    ")\n",
    "\n",
    "# Search with re-ranking\n",
    "reranked_results = rag.search_articles(\n",
    "    query=query,\n",
    "    n_results=5,\n",
    "    rerank=True\n",
    ")\n",
    "\n",
    "# Create a comparison table\n",
    "comparison_data = []\n",
    "\n",
    "for i in range(max(len(basic_results), len(reranked_results))):\n",
    "    basic_title = basic_results[i].get('title', '-') if i < len(basic_results) else '-'\n",
    "    basic_similarity = basic_results[i].get('similarity_score', 0) if i < len(basic_results) else 0\n",
    "    \n",
    "    reranked_title = reranked_results[i].get('title', '-') if i < len(reranked_results) else '-'\n",
    "    reranked_similarity = reranked_results[i].get('similarity_score', 0) if i < len(reranked_results) else 0\n",
    "    reranked_score = reranked_results[i].get('rerank_score', 0) if i < len(reranked_results) else 0\n",
    "    \n",
    "    comparison_data.append({\n",
    "        \"Rank\": i+1,\n",
    "        \"Basic Search Title\": basic_title,\n",
    "        \"Basic Similarity\": round(basic_similarity, 4),\n",
    "        \"Reranked Title\": reranked_title,\n",
    "        \"Reranked Similarity\": round(reranked_similarity, 4),\n",
    "        \"Rerank Score\": round(reranked_score, 4),\n",
    "    })\n",
    "\n",
    "# Display as a DataFrame\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00995043",
   "metadata": {},
   "source": [
    "## 6. Deleting Article Data\n",
    "\n",
    "If needed, we can delete articles and their associated embeddings from the databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b04b469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Delete an article by its URL hash\n",
    "# Note: You need to know the URL hash of an article to delete it\n",
    "# Get a URL hash from one of the search results\n",
    "if results and len(results) > 0:\n",
    "    article_to_delete = results[0]['url_hash']\n",
    "    print(f\"Deleting article with URL hash: {article_to_delete}\")\n",
    "    \n",
    "    result = rag.delete_article_data(article_to_delete)\n",
    "    \n",
    "    print(f\"\\nDelete operation completed with status: {result['status']}\")\n",
    "    print(f\"Message: {result['message']}\")\n",
    "    print(f\"Article deleted from SQLite: {result['article_deleted']}\")\n",
    "    print(f\"Embeddings deleted from ChromaDB: {result['embeddings_deleted']}\")\n",
    "else:\n",
    "    print(\"No articles available to demonstrate deletion.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c2e0f9",
   "metadata": {},
   "source": [
    "## 7. Clean Up\n",
    "\n",
    "When finished, it's important to properly close database connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59c94dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close database connections\n",
    "rag.close()\n",
    "print(\"Successfully closed all database connections.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c209f57",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've demonstrated the complete workflow using the `FinancialNewsRAG` orchestrator:\n",
    "\n",
    "1. **Initialization**: Setting up the orchestrator with API keys and configuration\n",
    "2. **Article Fetching**: Retrieving articles by tag and symbol from the EODHD API\n",
    "3. **Text Processing**: Cleaning and normalizing article content\n",
    "4. **Embedding Generation**: Creating vector representations of article chunks\n",
    "5. **Database Status**: Checking the state of both SQLite and ChromaDB\n",
    "6. **Semantic Search**: Finding relevant articles using natural language queries\n",
    "7. **Re-ranking**: Improving search results using the Gemini LLM\n",
    "8. **Data Management**: Deleting articles and embeddings when no longer needed\n",
    "\n",
    "The orchestrator provides a clean, high-level interface that simplifies the use of the financial-news-rag system, handling all the interactions between components and providing error handling and status reporting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
