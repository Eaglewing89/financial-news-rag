{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e940018d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "eodhd_api_key = os.getenv('EODHD_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee89946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = f'https://eodhd.com/api/news?t=venture%20capital&offset=1000&limit=1000&api_token={eodhd_api_key}&fmt=json'\n",
    "data = requests.get(url).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8c183a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed23046c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2576b505",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "# Assuming 'data' is your loaded JSON data\n",
    "# If you need to load from file instead, uncomment below:\n",
    "# with open('your_file.json', 'r') as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "def analyze_top_items(data, key_name, top_n=10):\n",
    "    \"\"\"Extract and count items from the specified key across all records.\"\"\"\n",
    "    # Flatten the list of lists\n",
    "    all_items = []\n",
    "    for item in data:\n",
    "        if key_name in item and isinstance(item[key_name], list):\n",
    "            all_items.extend(item[key_name])\n",
    "    \n",
    "    # Count occurrences\n",
    "    item_counts = Counter(all_items)\n",
    "    \n",
    "    # Get top N items\n",
    "    top_items = item_counts.most_common(top_n)\n",
    "    \n",
    "    return top_items\n",
    "\n",
    "# Get top 10 tags\n",
    "top_tags = analyze_top_items(data, 'tags', top_n=100)\n",
    "print(\"Top 10 Tags:\")\n",
    "for tag, count in top_tags:\n",
    "    print(f\"{tag}: {count}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "# Get top 10 symbols\n",
    "top_symbols = analyze_top_items(data, 'symbols')\n",
    "print(\"Top 10 Symbols:\")\n",
    "for symbol, count in top_symbols:\n",
    "    print(f\"{symbol}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8c21ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "def analyze_dates(data):\n",
    "    # Extract and parse dates\n",
    "    dates = [datetime.fromisoformat(item['date']) for item in data]\n",
    "    dates.sort()\n",
    "    \n",
    "    # Calculate time differences in hours between consecutive dates\n",
    "    diffs = [(dates[i+1] - dates[i]).total_seconds() / 3600 for i in range(len(dates)-1)]\n",
    "    \n",
    "    # Calculate statistics\n",
    "    mean_diff = np.mean(diffs) if diffs else 0\n",
    "    std_diff = np.std(diffs) if diffs else 0\n",
    "    \n",
    "    print(f\"Earliest date: {dates[0]}\")\n",
    "    print(f\"Latest date: {dates[-1]}\")\n",
    "    print(f\"Mean time between entries (hours): {mean_diff}\")\n",
    "    print(f\"Std deviation (hours): {std_diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baae0806",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_dates(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d002e2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "def analyze_dates(data):\n",
    "    # Extract and parse dates\n",
    "    dates = [datetime.fromisoformat(item['date']) for item in data]\n",
    "    dates.sort()\n",
    "    # Calculate time differences in hours between consecutive dates\n",
    "    diffs = [(dates[i+1] - dates[i]).total_seconds() / 3600 for i in range(len(dates)-1)]\n",
    "    mean_diff = np.mean(diffs) if diffs else 0\n",
    "    std_diff = np.std(diffs) if diffs else 0\n",
    "    return mean_diff, std_diff, diffs\n",
    "\n",
    "# Example usage for your four datasets:\n",
    "mean1, std1, diffs1 = analyze_dates(data)\n",
    "mean2, std2, diffs2 = analyze_dates(data2)\n",
    "mean3, std3, diffs3 = analyze_dates(data3)\n",
    "mean4, std4, diffs4 = analyze_dates(data4)\n",
    "\n",
    "# Combine all diffs for overall statistics\n",
    "all_diffs = diffs1 + diffs2 + diffs3 + diffs4\n",
    "overall_mean = np.mean(all_diffs) if all_diffs else 0\n",
    "overall_std = np.std(all_diffs) if all_diffs else 0\n",
    "\n",
    "print(f\"Mean1: {mean1}, Std1: {std1}\")\n",
    "print(f\"Mean2: {mean2}, Std2: {std2}\")\n",
    "print(f\"Mean3: {mean3}, Std3: {std3}\")\n",
    "print(f\"Mean4: {mean4}, Std4: {std4}\")\n",
    "print(f\"Overall mean (hours): {overall_mean}\")\n",
    "print(f\"Overall std (hours): {overall_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d93e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    a = np.array(data)\n",
    "    n = len(a)\n",
    "    mean = np.mean(a)\n",
    "    sem = stats.sem(a)  # Standard error of the mean\n",
    "    h = sem * stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return mean, mean-h, mean+h\n",
    "\n",
    "# Example usage:\n",
    "mean, lower, upper = mean_confidence_interval(all_diffs)\n",
    "print(f\"Mean: {mean:.2f} hours\")\n",
    "print(f\"95% confidence interval: [{lower:.2f}, {upper:.2f}] hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd7e53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def std_confidence_interval(data, confidence=0.95):\n",
    "    a = np.array(data)\n",
    "    n = len(a)\n",
    "    s = np.std(a, ddof=1)\n",
    "    alpha = 1 - confidence\n",
    "    chi2_lower = stats.chi2.ppf(alpha / 2, n - 1)\n",
    "    chi2_upper = stats.chi2.ppf(1 - alpha / 2, n - 1)\n",
    "    lower = np.sqrt((n - 1) * s**2 / chi2_upper)\n",
    "    upper = np.sqrt((n - 1) * s**2 / chi2_lower)\n",
    "    return s, lower, upper\n",
    "\n",
    "# Example usage:\n",
    "std, lower, upper = std_confidence_interval(all_diffs)\n",
    "print(f\"Std: {std:.2f} hours\")\n",
    "print(f\"95% confidence interval for std: [{lower:.2f}, {upper:.2f}] hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33df07e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_article_overlaps(data, data2, data3, data4, limit=50):\n",
    "    # Extract the first `limit` links from each list\n",
    "    links1 = set(d['link'] for d in data[:limit])\n",
    "    links2 = set(d['link'] for d in data2[:limit])\n",
    "    links3 = set(d['link'] for d in data3[:limit])\n",
    "    links4 = set(d['link'] for d in data4[:limit])\n",
    "\n",
    "    overlaps = {\n",
    "        'tech_business': links1 & links2,\n",
    "        'tech_ai': links1 & links3,\n",
    "        'tech_earnings': links1 & links4,\n",
    "        'business_ai': links2 & links3,\n",
    "        'business_earnings': links2 & links4,\n",
    "        'ai_earnings': links3 & links4,\n",
    "        'all_four': links1 & links2 & links3 & links4,\n",
    "    }\n",
    "    return overlaps\n",
    "\n",
    "# Example usage:\n",
    "overlaps = compare_article_overlaps(data, data2, data3, data4)\n",
    "print({k: len(v) for k, v in overlaps.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ce75e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def save_data(variable, filename):\n",
    "    \"\"\"Save a list of dicts to a JSON file inside the 'example_data' folder.\"\"\"\n",
    "    os.makedirs('example_data', exist_ok=True)\n",
    "    filepath = os.path.join('example_data', filename)\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(variable, f)\n",
    "\n",
    "def load_data(filename):\n",
    "    \"\"\"Load a list of dicts from a JSON file inside the 'example_data' folder.\"\"\"\n",
    "    filepath = os.path.join('example_data', filename)\n",
    "    with open(filepath, 'r') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bad89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "save_data(data, 'venture_capital.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9139bf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "data = load_data('data.json')\n",
    "data2 = load_data('data2.json')\n",
    "data3 = load_data('data3.json')\n",
    "data4 = load_data('data4.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
