{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2c99289",
   "metadata": {},
   "source": [
    "# FinancialNewsRAG Orchestrator Example\n",
    "This notebook demonstrates the functionality of the `FinancialNewsRAG` orchestrator class, which integrates various components of the financial news RAG system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55066cd2",
   "metadata": {},
   "source": [
    "## 1. Initialization\n",
    "First, we need to initialize the `FinancialNewsRAG` orchestrator. This requires API keys for EODHD and Gemini, which are automatically loaded from environment variables through the centralized `Config` class. You can also specify custom paths for the SQLite database and ChromaDB persistence directory.\n",
    "\n",
    "Make sure you have a `.env` file in your project root with `EODHD_API_KEY` and `GEMINI_API_KEY` set. Alternatively, you can pass these values directly to the constructor as overrides.\n",
    "\n",
    "### Centralized Configuration\n",
    "The system uses a centralized configuration approach where all settings are managed by the `Config` class in `config.py`. This class loads values from environment variables with sensible defaults. When initializing `FinancialNewsRAG`, any parameters you provide will override the values from the centralized configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6293fa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from financial_news_rag.orchestrator import FinancialNewsRAG\n",
    "\n",
    "# Initialize the orchestrator\n",
    "try:\n",
    "    # Using a unique DB path for this example to avoid conflicts with other uses\n",
    "    example_db_path = \"financial_news_rag_example.db\"\n",
    "    example_chroma_persist_dir = \"chroma_db_example\"\n",
    "    \n",
    "    # The orchestrator will automatically load API keys from the Config class,\n",
    "    # which reads from environment variables. If you want to override them, you can\n",
    "    # pass them directly as parameters:\n",
    "    # orchestrator = FinancialNewsRAG(\n",
    "    #     eodhd_api_key=\"your_eodhd_key_here\",\n",
    "    #     gemini_api_key=\"your_gemini_key_here\",\n",
    "    #     db_path=example_db_path,\n",
    "    #     chroma_persist_dir=example_chroma_persist_dir\n",
    "    # )\n",
    "    \n",
    "    # For this example, we'll just override the DB paths and let Config handle the API keys:\n",
    "    orchestrator = FinancialNewsRAG(\n",
    "        db_path=example_db_path,\n",
    "        chroma_persist_dir=example_chroma_persist_dir\n",
    "    )\n",
    "    \n",
    "    print(\"FinancialNewsRAG orchestrator initialized successfully.\")\n",
    "    print(f\"SQLite DB will be created/used at: {os.path.abspath(example_db_path)}\")\n",
    "    print(f\"ChromaDB will persist data in: {os.path.abspath(example_chroma_persist_dir)}\")\n",
    "except ValueError as e:\n",
    "    print(f\"Initialization failed: {e}\")\n",
    "    print(\"Please ensure EODHD_API_KEY and GEMINI_API_KEY are set in your .env file or environment variables.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9718f47",
   "metadata": {},
   "source": [
    "## 2. Fetching and Storing Articles\n",
    "The `fetch_and_store_articles` method fetches news articles from the EODHD API and stores them in the SQLite database. You can fetch articles by `tag` (e.g., 'TECHNOLOGY', 'M&A') or by `symbol` (e.g., 'AAPL.US'). You can also specify `from_date`, `to_date`, and a `limit` for the number of articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7722794",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'orchestrator' in locals():\n",
    "    # Example 1: Fetch articles by tag (e.g., 'M&A' for Mergers and Acquisitions)\n",
    "    # Using a very small limit for demonstration purposes\n",
    "    print(\"Fetching articles by tag 'MERGERS AND ACQUISITIONS' (Mergers and Acquisitions)...\")\n",
    "    fetch_results_tag = orchestrator.fetch_and_store_articles(tag=\"MERGERS AND ACQUISITIONS\", limit=20)\n",
    "    print(f\"Tag fetch results: {fetch_results_tag}\")\n",
    "    \n",
    "    # Example 2: Fetch articles by symbol (e.g., 'MSFT.US' for Microsoft)\n",
    "    print(\"Fetching articles by symbol 'MSFT.US'...\")\n",
    "    fetch_results_symbol = orchestrator.fetch_and_store_articles(symbol=\"MSFT.US\", limit=20)\n",
    "    print(f\"Symbol fetch results: {fetch_results_symbol}\")\n",
    "    \n",
    "    # Example 3: Fetch articles with a date range\n",
    "    # Note: EODHD free tier might have limitations on date ranges for news.\n",
    "    # Using a recent date range for better chances of getting results.\n",
    "    from datetime import datetime, timedelta\n",
    "    to_date_str = datetime.now().strftime('%Y-%m-%d')\n",
    "    from_date_str = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')\n",
    "    print(f\"Fetching articles for 'AAPL.US' from {from_date_str} to {to_date_str}...\")\n",
    "    fetch_results_date = orchestrator.fetch_and_store_articles(\n",
    "        symbol=\"AAPL.US\", \n",
    "        from_date=from_date_str, \n",
    "        to_date=to_date_str, \n",
    "        limit=20\n",
    "    )\n",
    "    print(f\"Date range fetch results: {fetch_results_date}\")\n",
    "else:\n",
    "    print(\"Orchestrator not initialized. Skipping fetch examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469377a6",
   "metadata": {},
   "source": [
    "## 3. Processing Articles\n",
    "The `process_articles_by_status` method retrieves articles from the database based on their processing status (e.g., 'PENDING', 'FAILED') and processes their raw content. Processing involves cleaning HTML, extracting text, and validating the content. Successfully processed articles are updated in the database with the cleaned content and a 'SUCCESS' status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996332ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'orchestrator' in locals():\n",
    "    # Process articles that are currently 'PENDING' (newly fetched)\n",
    "    print(\"Processing 'PENDING' articles...\")\n",
    "    processing_results_pending = orchestrator.process_articles_by_status(status='PENDING', limit=40)\n",
    "    print(f\"Pending processing results: {processing_results_pending}\")\n",
    "    \n",
    "    # Optionally, you could try to re-process 'FAILED' articles if any exist\n",
    "    # print(\"Attempting to re-process 'FAILED' articles...\")\n",
    "    # processing_results_failed = orchestrator.process_articles_by_status(status='FAILED', limit=5)\n",
    "    # print(f\"Failed processing results: {processing_results_failed}\")\n",
    "else:\n",
    "    print(\"Orchestrator not initialized. Skipping processing examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be877d8f",
   "metadata": {},
   "source": [
    "## 4. Embedding Articles\n",
    "The `embed_processed_articles` method takes articles that have been successfully processed and generates embeddings for their content. These embeddings are then stored in ChromaDB. This method can also handle articles whose previous embedding attempts were 'PENDING' or 'FAILED'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ae7e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'orchestrator' in locals():\n",
    "    # Embed articles whose content has been processed and are 'PENDING' embedding\n",
    "    print(\"Embedding 'PENDING' (embedding status) articles...\")\n",
    "    embedding_results_pending = orchestrator.embed_processed_articles(status='PENDING', limit=20)\n",
    "    print(f\"Pending embedding results: {embedding_results_pending}\")\n",
    "    \n",
    "    # Optionally, re-attempt embedding for articles that 'FAILED' previously\n",
    "    # print(\"Re-attempting to embed 'FAILED' (embedding status) articles...\")\n",
    "    # embedding_results_failed = orchestrator.embed_processed_articles(status='FAILED', limit=5)\n",
    "    # print(f\"Failed embedding results: {embedding_results_failed}\")\n",
    "else:\n",
    "    print(\"Orchestrator not initialized. Skipping embedding examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073c5abd",
   "metadata": {},
   "source": [
    "## 5. Database Status\n",
    "You can check the status of both the article database (SQLite) and the vector database (ChromaDB) using the following methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4556b3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'orchestrator' in locals():\n",
    "    # Get status of the article database (SQLite)\n",
    "    print(\"Fetching article database status...\")\n",
    "    article_db_status = orchestrator.get_article_database_status()\n",
    "    print(f\"Article DB Status: {article_db_status}\")\n",
    "    \n",
    "    # Get status of the vector database (ChromaDB)\n",
    "    print(\"Fetching vector database status...\")\n",
    "    vector_db_status = orchestrator.get_vector_database_status()\n",
    "    print(f\"Vector DB Status: {vector_db_status}\")\n",
    "else:\n",
    "    print(\"Orchestrator not initialized. Skipping database status examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbdbaf5",
   "metadata": {},
   "source": [
    "## 6. Searching Articles\n",
    "The `search_articles` method allows you to search for articles relevant to a given query. It generates an embedding for the query, searches ChromaDB for similar article chunks, retrieves the corresponding articles from SQLite, and can optionally re-rank the results using a Gemini LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f049b6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'orchestrator' in locals() and vector_db_status.get('total_chunks', 0) > 0:\n",
    "    # Example 1: Basic search\n",
    "    query1 = \"latest advancements in AI by Microsoft\"\n",
    "    print(f\"Searching for: '{query1}'...\")\n",
    "    search_results1 = orchestrator.search_articles(query=query1, n_results=5)\n",
    "    print(f\"Search Results 1 (basic):\")\n",
    "    for i, article in enumerate(search_results1):\n",
    "        print(f\"  {i+1}. Title: {article.get('title')}, Score: {article.get('similarity_score')}\")\n",
    "        # print(f\"     URL: {article.get('link')}\")\n",
    "        # print(f\"     Published: {article.get('published_at')}\")\n",
    "        # print(f\"     Content Snippet: {article.get('processed_content', '')[:200]}...\")\n",
    "    \n",
    "    # Example 2: Search with re-ranking\n",
    "    # Re-ranking can provide more contextually relevant results but is slower.\n",
    "    query2 = \"Acquisitions in the tech industry related to safety and security\"\n",
    "    print(f\"Searching for: '{query2}' with re-ranking...\")\n",
    "    search_results2 = orchestrator.search_articles(query=query2, n_results=5, rerank=True)\n",
    "    print(f\"Search Results 2 (re-ranked):\")\n",
    "    for i, article in enumerate(search_results2):\n",
    "        print(f\"  {i+1}. Title: {article.get('title')}, Score: {article.get('relevance_score', article.get('similarity_score'))}\") # ReRanker adds 'relevance_score'\n",
    "        print(f\"     URL: {article.get('url')}\")\n",
    "        print(f\"     Published: {article.get('published_at')}\")\n",
    "        print(f\"     Content Snippet: {article.get('processed_content', '')[:200]}...\")\n",
    "    \n",
    "    # Example 3: Search with date filtering\n",
    "    # Assuming some articles were published in the last few days\n",
    "    from_date_search = (datetime.now() - timedelta(days=3)).isoformat() + \"Z\" # ISO format\n",
    "    to_date_search = datetime.now().isoformat() + \"Z\"\n",
    "    query3 = \"market trends\"\n",
    "    print(f\"Searching for: '{query3}' between {from_date_search} and {to_date_search}...\")\n",
    "    search_results3 = orchestrator.search_articles(\n",
    "        query=query3, \n",
    "        n_results=5, \n",
    "        from_date_str=from_date_search, \n",
    "        to_date_str=to_date_search\n",
    "    )\n",
    "    print(f\"Search Results 3 (date filtered):\")\n",
    "    for i, article in enumerate(search_results3):\n",
    "        print(f\"  {i+1}. Title: {article.get('title')}, Published: {article.get('published_at')}, Score: {article.get('similarity_score')}\")\n",
    "elif 'orchestrator' in locals():\n",
    "    print(\"Vector database is empty. Skipping search examples. Please run fetch, process, and embed steps first.\")\n",
    "else:\n",
    "    print(\"Orchestrator not initialized. Skipping search examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267e3970",
   "metadata": {},
   "source": [
    "## 7. Deleting Old Articles\n",
    "The `delete_articles_older_than` method removes articles from both SQLite and ChromaDB that are older than a specified number of days. This is useful for managing data retention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe09292",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'orchestrator' in locals():\n",
    "    # Example: Delete articles older than 365 days\n",
    "    # For this example, it's unlikely to delete anything unless you've run this over a long period.\n",
    "    # We can try with a very small number of days to see if it targets the articles we just added,\n",
    "    # but be careful as this will actually delete them.\n",
    "    print(\"Attempting to delete articles older than 1 day (for demonstration)...\")\n",
    "    # This will likely target the articles fetched if they were published more than 1 day ago.\n",
    "    # If you want to keep them, use a larger number like 365.\n",
    "    delete_results = orchestrator.delete_articles_older_than(days=1) \n",
    "    print(f\"Deletion results: {delete_results}\")\n",
    "    \n",
    "    # Check status again after deletion\n",
    "    print(\"Fetching article database status after potential deletion...\")\n",
    "    article_db_status_after_delete = orchestrator.get_article_database_status()\n",
    "    print(f\"Article DB Status: {article_db_status_after_delete}\")\n",
    "    \n",
    "    print(\"Fetching vector database status after potential deletion...\")\n",
    "    vector_db_status_after_delete = orchestrator.get_vector_database_status()\n",
    "    print(f\"Vector DB Status: {vector_db_status_after_delete}\")\n",
    "else:\n",
    "    print(\"Orchestrator not initialized. Skipping deletion examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103a7b9b",
   "metadata": {},
   "source": [
    "## 8. Closing Connections\n",
    "Finally, the `close` method should be called to properly close database connections and release any other resources held by the orchestrator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a986b0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'orchestrator' in locals():\n",
    "    print(\"Closing orchestrator connections...\")\n",
    "    orchestrator.close()\n",
    "    print(\"Orchestrator connections closed.\")\n",
    "    \n",
    "    # Clean up the example database and chroma directory created by this notebook\n",
    "    # You might want to comment this out if you want to inspect the files afterwards\n",
    "    if os.path.exists(example_db_path):\n",
    "        os.remove(example_db_path)\n",
    "        print(f\"Removed example SQLite DB: {example_db_path}\")\n",
    "    if os.path.exists(example_chroma_persist_dir):\n",
    "        import shutil\n",
    "        shutil.rmtree(example_chroma_persist_dir)\n",
    "        print(f\"Removed example ChromaDB directory: {example_chroma_persist_dir}\")\n",
    "else:\n",
    "    print(\"Orchestrator not initialized. Skipping close example.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e07ca81",
   "metadata": {},
   "source": [
    "## 9. Customizing Configuration\n",
    "\n",
    "The system uses a centralized configuration approach through the `Config` class. There are several ways to customize the configuration:\n",
    "\n",
    "1. **Environment Variables**: Set variables in your `.env` file or environment\n",
    "2. **Direct Overrides**: Pass parameters directly to the `FinancialNewsRAG` constructor\n",
    "3. **Custom Config Class**: Create a subclass of `Config` with your own settings\n",
    "\n",
    "Here's an example of how to override configuration values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486badd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of creating an orchestrator with custom configuration\n",
    "# (This is just for demonstration - don't actually run this code as it would create another instance)\n",
    "\n",
    "'''\n",
    "# Method 1: Override via constructor parameters\n",
    "custom_orchestrator = FinancialNewsRAG(\n",
    "    # API keys (override environment variables)\n",
    "    eodhd_api_key=\"your_custom_eodhd_key\",\n",
    "    gemini_api_key=\"your_custom_gemini_key\",\n",
    "    \n",
    "    # Database paths\n",
    "    db_path=\"custom_database.db\",\n",
    "    chroma_persist_dir=\"custom_chroma_dir\",\n",
    "    \n",
    "    # Text processing settings\n",
    "    max_tokens_per_chunk=1024  # Override the default chunk size\n",
    ")\n",
    "\n",
    "# Method 2: Set environment variables before importing\n",
    "# You can set these in your .env file or programmatically:\n",
    "import os\n",
    "os.environ['EODHD_API_KEY'] = 'your_api_key'\n",
    "os.environ['GEMINI_API_KEY'] = 'your_gemini_key'\n",
    "os.environ['TEXTPROCESSOR_MAX_TOKENS_PER_CHUNK'] = '1024'\n",
    "os.environ['CHROMA_DEFAULT_COLLECTION_NAME'] = 'my_custom_collection'\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ba2af0",
   "metadata": {},
   "source": [
    "This concludes the demonstration of the `FinancialNewsRAG` orchestrator. You can adapt these examples to build more complex workflows for your financial news analysis tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
